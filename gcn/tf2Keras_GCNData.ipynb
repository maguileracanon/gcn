{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import scipy.sparse as sp\n",
    "import networkx as nx\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "\n",
    "####################################################################\n",
    "#################INIZIALISATION ###############\n",
    "\n",
    "conv=16\n",
    "\n",
    "####################################################################\n",
    "\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data('cora')\n",
    "adjfeatures_arr=sparse_to_tuple(features)\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "features=features.toarray()\n",
    "\n",
    "###############################################################################################################################################################\n",
    "###############################################################################################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj as sparse tensor\n",
    "adj_tf=tf.sparse.SparseTensor(support[0], support[1], support[2])\n",
    "adj_tf=tf.dtypes.cast(adj_tf,tf.float32) \n",
    "#adj_tf=tf.sparse.to_dense(tf.sparse.reorder(adj_tf))\n",
    "#features as sparse tensor\n",
    "features_tf=tf.convert_to_tensor(features, dtype=tf.float32)\n",
    "# cast to tensorflow all arr\n",
    "y_train_tf=tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
    "train_mask_tf=tf.convert_to_tensor(train_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2708,), dtype=bool, numpy=array([ True,  True,  True, ..., False, False, False])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mask_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GCNLayer\n",
    "import sonnet as snt\n",
    "learning_rate=0.01\n",
    "\n",
    "model=GCNLayer.GCN(convolution_kernel_1=conv,convolution_kernel_2=np.shape(y_train)[1])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "from metrics import masked_softmax_cross_entropy\n",
    "\n",
    "def ComputeLoss(labels, preds,mask):\n",
    "    _loss=masked_softmax_cross_entropy(preds, labels, mask)\n",
    "    _masked=_loss\n",
    "    _loss+= tf.nn.l2_loss(model.trainable_variables[-1])*5e-4\n",
    "    return _loss,_masked\n",
    "\n",
    "\n",
    "def update_step(inputs_tr,adj, targets_tr,masks):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output_ops_tr = model(inputs_tr,adj)\n",
    "        # Loss.\n",
    "        loss_tr,masked_tr = ComputeLoss(targets_tr,output_ops_tr,masks)\n",
    "        \n",
    "\n",
    "    gradients = tape.gradient(loss_tr, model.trainable_variables)\n",
    "    #grads = tf.distribute.get_replica_context().all_reduce('sum', gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    #optimizer.apply(gradients, model.trainable_variables)\n",
    "    return loss_tr,masked_tr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_signature = [tf.TensorSpec.from_tensor(features_tf),tf.SparseTensorSpec(shape=adj_tf.get_shape(),dtype=tf.dtypes.float32),tf.TensorSpec.from_tensor(y_train_tf),tf.TensorSpec.from_tensor(train_mask_tf)]\n",
    "compiled_update_step = tf.function(update_step, input_signature=input_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.96274\n",
      "Epoch: 0002 train_loss= 1.84639\n",
      "Epoch: 0003 train_loss= 1.73923\n",
      "Epoch: 0004 train_loss= 1.58959\n",
      "Epoch: 0005 train_loss= 1.44386\n",
      "Epoch: 0006 train_loss= 1.35270\n",
      "Epoch: 0007 train_loss= 1.19640\n",
      "Epoch: 0008 train_loss= 1.07133\n",
      "Epoch: 0009 train_loss= 0.97654\n",
      "Epoch: 0010 train_loss= 0.92415\n",
      "Epoch: 0011 train_loss= 0.84775\n",
      "Epoch: 0012 train_loss= 0.71689\n",
      "Epoch: 0013 train_loss= 0.63866\n",
      "Epoch: 0014 train_loss= 0.62385\n",
      "Epoch: 0015 train_loss= 0.50535\n",
      "Epoch: 0016 train_loss= 0.49537\n",
      "Epoch: 0017 train_loss= 0.48774\n",
      "Epoch: 0018 train_loss= 0.44343\n",
      "Epoch: 0019 train_loss= 0.41045\n",
      "Epoch: 0020 train_loss= 0.36089\n",
      "Epoch: 0021 train_loss= 0.30845\n",
      "Epoch: 0022 train_loss= 0.29369\n",
      "Epoch: 0023 train_loss= 0.32588\n",
      "Epoch: 0024 train_loss= 0.27005\n",
      "Epoch: 0025 train_loss= 0.27608\n",
      "Epoch: 0026 train_loss= 0.21855\n",
      "Epoch: 0027 train_loss= 0.19943\n",
      "Epoch: 0028 train_loss= 0.19729\n",
      "Epoch: 0029 train_loss= 0.16722\n",
      "Epoch: 0030 train_loss= 0.17609\n",
      "Epoch: 0031 train_loss= 0.16461\n",
      "Epoch: 0032 train_loss= 0.16107\n",
      "Epoch: 0033 train_loss= 0.15585\n",
      "Epoch: 0034 train_loss= 0.15588\n",
      "Epoch: 0035 train_loss= 0.12281\n",
      "Epoch: 0036 train_loss= 0.10813\n",
      "Epoch: 0037 train_loss= 0.12208\n",
      "Epoch: 0038 train_loss= 0.11010\n",
      "Epoch: 0039 train_loss= 0.09784\n",
      "Epoch: 0040 train_loss= 0.10135\n",
      "Epoch: 0041 train_loss= 0.09895\n",
      "Epoch: 0042 train_loss= 0.10888\n",
      "Epoch: 0043 train_loss= 0.10228\n",
      "Epoch: 0044 train_loss= 0.08148\n",
      "Epoch: 0045 train_loss= 0.07036\n",
      "Epoch: 0046 train_loss= 0.08299\n",
      "Epoch: 0047 train_loss= 0.06989\n",
      "Epoch: 0048 train_loss= 0.09303\n",
      "Epoch: 0049 train_loss= 0.09305\n",
      "Epoch: 0050 train_loss= 0.07264\n",
      "Epoch: 0051 train_loss= 0.08090\n",
      "Epoch: 0052 train_loss= 0.05943\n",
      "Epoch: 0053 train_loss= 0.06641\n",
      "Epoch: 0054 train_loss= 0.05701\n",
      "Epoch: 0055 train_loss= 0.08734\n",
      "Epoch: 0056 train_loss= 0.08564\n",
      "Epoch: 0057 train_loss= 0.06721\n",
      "Epoch: 0058 train_loss= 0.06922\n",
      "Epoch: 0059 train_loss= 0.05529\n",
      "Epoch: 0060 train_loss= 0.08022\n",
      "Epoch: 0061 train_loss= 0.04750\n",
      "Epoch: 0062 train_loss= 0.05338\n",
      "Epoch: 0063 train_loss= 0.05004\n",
      "Epoch: 0064 train_loss= 0.09862\n",
      "Epoch: 0065 train_loss= 0.06706\n",
      "Epoch: 0066 train_loss= 0.07312\n",
      "Epoch: 0067 train_loss= 0.06861\n",
      "Epoch: 0068 train_loss= 0.08368\n",
      "Epoch: 0069 train_loss= 0.05913\n",
      "Epoch: 0070 train_loss= 0.07382\n",
      "Epoch: 0071 train_loss= 0.05476\n",
      "Epoch: 0072 train_loss= 0.05601\n",
      "Epoch: 0073 train_loss= 0.06572\n",
      "Epoch: 0074 train_loss= 0.05364\n",
      "Epoch: 0075 train_loss= 0.04832\n",
      "Epoch: 0076 train_loss= 0.05150\n",
      "Epoch: 0077 train_loss= 0.05439\n",
      "Epoch: 0078 train_loss= 0.04192\n",
      "Epoch: 0079 train_loss= 0.04460\n",
      "Epoch: 0080 train_loss= 0.04959\n",
      "Epoch: 0081 train_loss= 0.06502\n",
      "Epoch: 0082 train_loss= 0.05164\n",
      "Epoch: 0083 train_loss= 0.06722\n",
      "Epoch: 0084 train_loss= 0.04366\n",
      "Epoch: 0085 train_loss= 0.04842\n",
      "Epoch: 0086 train_loss= 0.06360\n",
      "Epoch: 0087 train_loss= 0.02950\n",
      "Epoch: 0088 train_loss= 0.03978\n",
      "Epoch: 0089 train_loss= 0.04085\n",
      "Epoch: 0090 train_loss= 0.05476\n",
      "Epoch: 0091 train_loss= 0.07774\n",
      "Epoch: 0092 train_loss= 0.04322\n",
      "Epoch: 0093 train_loss= 0.05348\n",
      "Epoch: 0094 train_loss= 0.04598\n",
      "Epoch: 0095 train_loss= 0.04406\n",
      "Epoch: 0096 train_loss= 0.03643\n",
      "Epoch: 0097 train_loss= 0.07720\n",
      "Epoch: 0098 train_loss= 0.05115\n",
      "Epoch: 0099 train_loss= 0.04933\n",
      "Epoch: 0100 train_loss= 0.04232\n",
      "Epoch: 0101 train_loss= 0.03369\n",
      "Epoch: 0102 train_loss= 0.05654\n",
      "Epoch: 0103 train_loss= 0.04778\n",
      "Epoch: 0104 train_loss= 0.04210\n",
      "Epoch: 0105 train_loss= 0.03364\n",
      "Epoch: 0106 train_loss= 0.05030\n",
      "Epoch: 0107 train_loss= 0.02691\n",
      "Epoch: 0108 train_loss= 0.05287\n",
      "Epoch: 0109 train_loss= 0.03894\n",
      "Epoch: 0110 train_loss= 0.02960\n",
      "Epoch: 0111 train_loss= 0.03859\n",
      "Epoch: 0112 train_loss= 0.03812\n",
      "Epoch: 0113 train_loss= 0.05147\n",
      "Epoch: 0114 train_loss= 0.04357\n",
      "Epoch: 0115 train_loss= 0.03079\n",
      "Epoch: 0116 train_loss= 0.02916\n",
      "Epoch: 0117 train_loss= 0.04353\n",
      "Epoch: 0118 train_loss= 0.06763\n",
      "Epoch: 0119 train_loss= 0.04062\n",
      "Epoch: 0120 train_loss= 0.05649\n",
      "Epoch: 0121 train_loss= 0.07060\n",
      "Epoch: 0122 train_loss= 0.03144\n",
      "Epoch: 0123 train_loss= 0.03186\n",
      "Epoch: 0124 train_loss= 0.04730\n",
      "Epoch: 0125 train_loss= 0.03810\n",
      "Epoch: 0126 train_loss= 0.04583\n",
      "Epoch: 0127 train_loss= 0.03369\n",
      "Epoch: 0128 train_loss= 0.03242\n",
      "Epoch: 0129 train_loss= 0.03025\n",
      "Epoch: 0130 train_loss= 0.03623\n",
      "Epoch: 0131 train_loss= 0.02890\n",
      "Epoch: 0132 train_loss= 0.04635\n",
      "Epoch: 0133 train_loss= 0.02844\n",
      "Epoch: 0134 train_loss= 0.03113\n",
      "Epoch: 0135 train_loss= 0.03065\n",
      "Epoch: 0136 train_loss= 0.04817\n",
      "Epoch: 0137 train_loss= 0.03042\n",
      "Epoch: 0138 train_loss= 0.04085\n",
      "Epoch: 0139 train_loss= 0.02581\n",
      "Epoch: 0140 train_loss= 0.05259\n",
      "Epoch: 0141 train_loss= 0.04885\n",
      "Epoch: 0142 train_loss= 0.03457\n",
      "Epoch: 0143 train_loss= 0.06118\n",
      "Epoch: 0144 train_loss= 0.03107\n",
      "Epoch: 0145 train_loss= 0.03003\n",
      "Epoch: 0146 train_loss= 0.02757\n",
      "Epoch: 0147 train_loss= 0.02999\n",
      "Epoch: 0148 train_loss= 0.04638\n",
      "Epoch: 0149 train_loss= 0.03699\n",
      "Epoch: 0150 train_loss= 0.03578\n",
      "Epoch: 0151 train_loss= 0.02517\n",
      "Epoch: 0152 train_loss= 0.02552\n",
      "Epoch: 0153 train_loss= 0.02495\n",
      "Epoch: 0154 train_loss= 0.02883\n",
      "Epoch: 0155 train_loss= 0.02497\n",
      "Epoch: 0156 train_loss= 0.02392\n",
      "Epoch: 0157 train_loss= 0.03290\n",
      "Epoch: 0158 train_loss= 0.03500\n",
      "Epoch: 0159 train_loss= 0.03221\n",
      "Epoch: 0160 train_loss= 0.03324\n",
      "Epoch: 0161 train_loss= 0.02486\n",
      "Epoch: 0162 train_loss= 0.05162\n",
      "Epoch: 0163 train_loss= 0.03991\n",
      "Epoch: 0164 train_loss= 0.02751\n",
      "Epoch: 0165 train_loss= 0.04099\n",
      "Epoch: 0166 train_loss= 0.02853\n",
      "Epoch: 0167 train_loss= 0.02642\n",
      "Epoch: 0168 train_loss= 0.03296\n",
      "Epoch: 0169 train_loss= 0.02184\n",
      "Epoch: 0170 train_loss= 0.05131\n",
      "Epoch: 0171 train_loss= 0.03742\n",
      "Epoch: 0172 train_loss= 0.03690\n",
      "Epoch: 0173 train_loss= 0.03525\n",
      "Epoch: 0174 train_loss= 0.03243\n",
      "Epoch: 0175 train_loss= 0.02590\n",
      "Epoch: 0176 train_loss= 0.03172\n",
      "Epoch: 0177 train_loss= 0.02440\n",
      "Epoch: 0178 train_loss= 0.02920\n",
      "Epoch: 0179 train_loss= 0.02720\n",
      "Epoch: 0180 train_loss= 0.02295\n",
      "Epoch: 0181 train_loss= 0.02248\n",
      "Epoch: 0182 train_loss= 0.03339\n",
      "Epoch: 0183 train_loss= 0.03076\n",
      "Epoch: 0184 train_loss= 0.02991\n",
      "Epoch: 0185 train_loss= 0.02259\n",
      "Epoch: 0186 train_loss= 0.02273\n",
      "Epoch: 0187 train_loss= 0.03336\n",
      "Epoch: 0188 train_loss= 0.02432\n",
      "Epoch: 0189 train_loss= 0.02418\n",
      "Epoch: 0190 train_loss= 0.02896\n",
      "Epoch: 0191 train_loss= 0.03900\n",
      "Epoch: 0192 train_loss= 0.02987\n",
      "Epoch: 0193 train_loss= 0.02727\n",
      "Epoch: 0194 train_loss= 0.02825\n",
      "Epoch: 0195 train_loss= 0.02899\n",
      "Epoch: 0196 train_loss= 0.02229\n",
      "Epoch: 0197 train_loss= 0.02620\n",
      "Epoch: 0198 train_loss= 0.03697\n",
      "Epoch: 0199 train_loss= 0.02514\n",
      "Epoch: 0200 train_loss= 0.02862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train model\n",
    "for epoch in range(200):\n",
    "    loss_tr,masked_tr = compiled_update_step(features_tf, adj_tf, y_train_tf,train_mask_tf)\n",
    "    \n",
    "    if(epoch%1==0):\n",
    "        print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(loss_tr.numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06629568"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_tr.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn_Sonnet",
   "language": "python",
   "name": "gcn_sonnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
