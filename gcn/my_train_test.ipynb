{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from gcn.utils import *\n",
    "\n",
    "\n",
    "\n",
    "# Set random seed\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "flags.DEFINE_string('dataset', 'cora', 'Dataset string.')  # 'cora', 'citeseer', 'pubmed'\n",
    "flags.DEFINE_string('model', 'gcn', 'Model string.')  # 'gcn', 'gcn_cheby', 'dense'\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('epochs', 200, 'Number of epochs to train.')\n",
    "flags.DEFINE_integer('hidden1', 16, 'Number of units in hidden layer 1.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Dropout rate (1 - keep probability).')\n",
    "flags.DEFINE_float('weight_decay', 5e-4, 'Weight for L2 loss on embedding matrix.')\n",
    "flags.DEFINE_integer('early_stopping', 10, 'Tolerance for early stopping (# of epochs).')\n",
    "flags.DEFINE_integer('max_degree', 3, 'Maximum Chebyshev polynomial degree.')\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjfeatures_arr=features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = [preprocess_adj(adj)]\n",
    "\n",
    "num_supports = 1\n",
    "\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.placeholder(tf.float32, shape=(None, adjfeatures_arr.shape[1])),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn.layers import *\n",
    "from gcn.metrics import *\n",
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.cross_ent=0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "        \n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "        \n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        print(\"lalal\")\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        \n",
    "        self.cross_ent = masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "        self.loss += self.cross_ent\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=False,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        print(\"Function called during training\")\n",
    "        return tf.nn.softmax(self.outputs)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mara/anaconda3/envs/gcn/lib/python3.6/site-packages/gcn/inits.py:14: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mara/anaconda3/envs/gcn/lib/python3.6/site-packages/gcn/layers.py:82: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/mara/anaconda3/envs/gcn/lib/python3.6/site-packages/gcn/layers.py:170: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/mara/anaconda3/envs/gcn/lib/python3.6/site-packages/gcn/layers.py:33: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
      "\n",
      "lalal\n",
      "WARNING:tensorflow:From /home/mara/anaconda3/envs/gcn/lib/python3.6/site-packages/gcn/metrics.py:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GCN(placeholders, input_dim=np.shape(adjfeatures_arr)[1], logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.95137 train_acc= 0.10714 Cross_ent= 1.94353 val_acc= 0.36000 time= 0.16782\n",
      "Epoch: 0002 train_loss= 1.87833 train_acc= 0.32143 Cross_ent= 1.87097 val_acc= 0.43400 time= 0.05997\n",
      "Epoch: 0003 train_loss= 1.79982 train_acc= 0.43571 Cross_ent= 1.79225 val_acc= 0.48400 time= 0.06018\n",
      "Epoch: 0004 train_loss= 1.68983 train_acc= 0.60000 Cross_ent= 1.68165 val_acc= 0.52600 time= 0.05502\n",
      "Epoch: 0005 train_loss= 1.60888 train_acc= 0.55714 Cross_ent= 1.59973 val_acc= 0.54000 time= 0.05262\n",
      "Epoch: 0006 train_loss= 1.45959 train_acc= 0.63571 Cross_ent= 1.44914 val_acc= 0.55000 time= 0.04953\n",
      "Epoch: 0007 train_loss= 1.37735 train_acc= 0.59286 Cross_ent= 1.36529 val_acc= 0.57200 time= 0.05558\n",
      "Epoch: 0008 train_loss= 1.30527 train_acc= 0.60714 Cross_ent= 1.29131 val_acc= 0.59000 time= 0.06087\n",
      "Epoch: 0009 train_loss= 1.21058 train_acc= 0.70000 Cross_ent= 1.19446 val_acc= 0.63000 time= 0.05615\n",
      "Epoch: 0010 train_loss= 1.09796 train_acc= 0.75000 Cross_ent= 1.07942 val_acc= 0.66200 time= 0.04951\n",
      "Epoch: 0011 train_loss= 1.01304 train_acc= 0.80714 Cross_ent= 0.99186 val_acc= 0.69200 time= 0.04947\n",
      "Epoch: 0012 train_loss= 0.93794 train_acc= 0.79286 Cross_ent= 0.91390 val_acc= 0.71800 time= 0.05573\n",
      "Epoch: 0013 train_loss= 0.87703 train_acc= 0.82857 Cross_ent= 0.84994 val_acc= 0.74200 time= 0.04872\n",
      "Epoch: 0014 train_loss= 0.79384 train_acc= 0.86429 Cross_ent= 0.76354 val_acc= 0.75800 time= 0.05185\n",
      "Epoch: 0015 train_loss= 0.71588 train_acc= 0.91429 Cross_ent= 0.68225 val_acc= 0.78000 time= 0.05097\n",
      "Epoch: 0016 train_loss= 0.65422 train_acc= 0.92857 Cross_ent= 0.61716 val_acc= 0.78200 time= 0.05713\n",
      "Epoch: 0017 train_loss= 0.61731 train_acc= 0.93571 Cross_ent= 0.57672 val_acc= 0.78000 time= 0.04947\n",
      "Epoch: 0018 train_loss= 0.60615 train_acc= 0.92143 Cross_ent= 0.56198 val_acc= 0.78400 time= 0.05656\n",
      "Epoch: 0019 train_loss= 0.49194 train_acc= 0.95000 Cross_ent= 0.44416 val_acc= 0.78600 time= 0.04973\n",
      "Epoch: 0020 train_loss= 0.43915 train_acc= 0.95714 Cross_ent= 0.38779 val_acc= 0.78200 time= 0.04795\n",
      "Epoch: 0021 train_loss= 0.42960 train_acc= 0.95000 Cross_ent= 0.37469 val_acc= 0.78200 time= 0.04686\n",
      "Epoch: 0022 train_loss= 0.36664 train_acc= 0.94286 Cross_ent= 0.30825 val_acc= 0.78000 time= 0.04830\n",
      "Epoch: 0023 train_loss= 0.40967 train_acc= 0.93571 Cross_ent= 0.34789 val_acc= 0.77400 time= 0.04933\n",
      "Epoch: 0024 train_loss= 0.37245 train_acc= 0.97857 Cross_ent= 0.30738 val_acc= 0.77200 time= 0.06828\n",
      "Epoch: 0025 train_loss= 0.34111 train_acc= 0.96429 Cross_ent= 0.27285 val_acc= 0.77000 time= 0.05818\n",
      "Epoch: 0026 train_loss= 0.33495 train_acc= 0.97143 Cross_ent= 0.26362 val_acc= 0.76800 time= 0.05961\n",
      "Epoch: 0027 train_loss= 0.35085 train_acc= 0.92857 Cross_ent= 0.27659 val_acc= 0.76800 time= 0.06508\n",
      "Epoch: 0028 train_loss= 0.28688 train_acc= 0.97143 Cross_ent= 0.20982 val_acc= 0.77000 time= 0.05151\n",
      "Epoch: 0029 train_loss= 0.30029 train_acc= 0.95714 Cross_ent= 0.22059 val_acc= 0.76800 time= 0.05458\n",
      "Epoch: 0030 train_loss= 0.29578 train_acc= 0.95714 Cross_ent= 0.21361 val_acc= 0.76800 time= 0.04902\n",
      "Epoch: 0031 train_loss= 0.27053 train_acc= 0.96429 Cross_ent= 0.18604 val_acc= 0.76800 time= 0.05264\n",
      "Epoch: 0032 train_loss= 0.26547 train_acc= 0.98571 Cross_ent= 0.17883 val_acc= 0.77200 time= 0.06114\n",
      "Epoch: 0033 train_loss= 0.27069 train_acc= 0.95714 Cross_ent= 0.18204 val_acc= 0.77200 time= 0.06067\n",
      "Epoch: 0034 train_loss= 0.27556 train_acc= 0.95714 Cross_ent= 0.18506 val_acc= 0.77200 time= 0.06550\n",
      "Epoch: 0035 train_loss= 0.27742 train_acc= 0.94286 Cross_ent= 0.18525 val_acc= 0.77200 time= 0.05492\n",
      "Epoch: 0036 train_loss= 0.23962 train_acc= 0.98571 Cross_ent= 0.14592 val_acc= 0.77200 time= 0.06027\n",
      "Epoch: 0037 train_loss= 0.25713 train_acc= 0.97857 Cross_ent= 0.16206 val_acc= 0.77200 time= 0.05155\n",
      "Epoch: 0038 train_loss= 0.21328 train_acc= 0.97857 Cross_ent= 0.11699 val_acc= 0.77200 time= 0.05448\n",
      "Epoch: 0039 train_loss= 0.19076 train_acc= 0.99286 Cross_ent= 0.09339 val_acc= 0.77000 time= 0.05636\n",
      "Epoch: 0040 train_loss= 0.25441 train_acc= 0.96429 Cross_ent= 0.15611 val_acc= 0.77000 time= 0.05637\n",
      "Epoch: 0041 train_loss= 0.22903 train_acc= 0.99286 Cross_ent= 0.12993 val_acc= 0.77000 time= 0.06658\n",
      "Epoch: 0042 train_loss= 0.20873 train_acc= 1.00000 Cross_ent= 0.10894 val_acc= 0.77000 time= 0.07032\n",
      "Epoch: 0043 train_loss= 0.20071 train_acc= 0.98571 Cross_ent= 0.10034 val_acc= 0.77000 time= 0.04876\n",
      "Epoch: 0044 train_loss= 0.21686 train_acc= 0.97857 Cross_ent= 0.11601 val_acc= 0.76600 time= 0.05698\n",
      "Epoch: 0045 train_loss= 0.23443 train_acc= 0.96429 Cross_ent= 0.13320 val_acc= 0.76800 time= 0.05549\n",
      "Epoch: 0046 train_loss= 0.20480 train_acc= 1.00000 Cross_ent= 0.10325 val_acc= 0.77200 time= 0.06434\n",
      "Epoch: 0047 train_loss= 0.17949 train_acc= 0.98571 Cross_ent= 0.07771 val_acc= 0.77400 time= 0.05934\n",
      "Epoch: 0048 train_loss= 0.20848 train_acc= 0.97857 Cross_ent= 0.10657 val_acc= 0.77400 time= 0.05934\n",
      "Epoch: 0049 train_loss= 0.21676 train_acc= 0.97857 Cross_ent= 0.11477 val_acc= 0.77400 time= 0.06729\n",
      "Epoch: 0050 train_loss= 0.18530 train_acc= 0.98571 Cross_ent= 0.08330 val_acc= 0.77600 time= 0.07001\n",
      "Epoch: 0051 train_loss= 0.19304 train_acc= 0.98571 Cross_ent= 0.09111 val_acc= 0.77800 time= 0.06025\n",
      "Epoch: 0052 train_loss= 0.20413 train_acc= 0.99286 Cross_ent= 0.10233 val_acc= 0.77800 time= 0.06325\n",
      "Epoch: 0053 train_loss= 0.19889 train_acc= 0.98571 Cross_ent= 0.09726 val_acc= 0.78200 time= 0.06469\n",
      "Epoch: 0054 train_loss= 0.18909 train_acc= 0.98571 Cross_ent= 0.08766 val_acc= 0.78400 time= 0.06441\n",
      "Epoch: 0055 train_loss= 0.18099 train_acc= 0.98571 Cross_ent= 0.07980 val_acc= 0.77800 time= 0.06798\n",
      "Epoch: 0056 train_loss= 0.15890 train_acc= 0.99286 Cross_ent= 0.05800 val_acc= 0.77800 time= 0.05586\n",
      "Epoch: 0057 train_loss= 0.17991 train_acc= 0.99286 Cross_ent= 0.07937 val_acc= 0.77600 time= 0.06005\n",
      "Epoch: 0058 train_loss= 0.21292 train_acc= 0.98571 Cross_ent= 0.11277 val_acc= 0.77800 time= 0.05771\n",
      "Epoch: 0059 train_loss= 0.18954 train_acc= 0.97857 Cross_ent= 0.08980 val_acc= 0.77800 time= 0.05383\n",
      "Epoch: 0060 train_loss= 0.16381 train_acc= 0.99286 Cross_ent= 0.06450 val_acc= 0.77800 time= 0.05539\n",
      "Epoch: 0061 train_loss= 0.16557 train_acc= 0.99286 Cross_ent= 0.06674 val_acc= 0.77400 time= 0.04771\n",
      "Epoch: 0062 train_loss= 0.19835 train_acc= 0.98571 Cross_ent= 0.10002 val_acc= 0.77400 time= 0.05022\n",
      "Epoch: 0063 train_loss= 0.18381 train_acc= 0.99286 Cross_ent= 0.08597 val_acc= 0.77400 time= 0.05604\n",
      "Epoch: 0064 train_loss= 0.16064 train_acc= 0.98571 Cross_ent= 0.06329 val_acc= 0.77400 time= 0.04948\n",
      "Epoch: 0065 train_loss= 0.16771 train_acc= 0.99286 Cross_ent= 0.07089 val_acc= 0.77400 time= 0.05981\n",
      "Epoch: 0066 train_loss= 0.17887 train_acc= 0.98571 Cross_ent= 0.08258 val_acc= 0.77200 time= 0.05132\n",
      "Epoch: 0067 train_loss= 0.13269 train_acc= 1.00000 Cross_ent= 0.03694 val_acc= 0.78000 time= 0.05420\n",
      "Epoch: 0068 train_loss= 0.17664 train_acc= 0.98571 Cross_ent= 0.08147 val_acc= 0.78200 time= 0.05276\n",
      "Epoch: 0069 train_loss= 0.16054 train_acc= 0.97857 Cross_ent= 0.06597 val_acc= 0.78200 time= 0.05952\n",
      "Epoch: 0070 train_loss= 0.15981 train_acc= 1.00000 Cross_ent= 0.06587 val_acc= 0.78200 time= 0.06474\n",
      "Epoch: 0071 train_loss= 0.16309 train_acc= 1.00000 Cross_ent= 0.06978 val_acc= 0.78000 time= 0.05785\n",
      "Epoch: 0072 train_loss= 0.15834 train_acc= 1.00000 Cross_ent= 0.06562 val_acc= 0.78200 time= 0.05871\n",
      "Epoch: 0073 train_loss= 0.13982 train_acc= 0.99286 Cross_ent= 0.04769 val_acc= 0.78400 time= 0.05660\n",
      "Epoch: 0074 train_loss= 0.14340 train_acc= 1.00000 Cross_ent= 0.05187 val_acc= 0.78800 time= 0.05495\n",
      "Epoch: 0075 train_loss= 0.14343 train_acc= 0.99286 Cross_ent= 0.05251 val_acc= 0.78600 time= 0.05895\n",
      "Epoch: 0076 train_loss= 0.16043 train_acc= 0.99286 Cross_ent= 0.07013 val_acc= 0.78600 time= 0.05042\n",
      "Epoch: 0077 train_loss= 0.14820 train_acc= 1.00000 Cross_ent= 0.05852 val_acc= 0.78200 time= 0.04736\n",
      "Epoch: 0078 train_loss= 0.16316 train_acc= 0.99286 Cross_ent= 0.07408 val_acc= 0.78200 time= 0.05937\n",
      "Epoch: 0079 train_loss= 0.16906 train_acc= 0.99286 Cross_ent= 0.08062 val_acc= 0.78000 time= 0.05547\n",
      "Epoch: 0080 train_loss= 0.15970 train_acc= 0.98571 Cross_ent= 0.07188 val_acc= 0.78200 time= 0.05409\n",
      "Epoch: 0081 train_loss= 0.14159 train_acc= 1.00000 Cross_ent= 0.05436 val_acc= 0.78000 time= 0.05426\n",
      "Epoch: 0082 train_loss= 0.13645 train_acc= 1.00000 Cross_ent= 0.04982 val_acc= 0.77800 time= 0.04855\n",
      "Epoch: 0083 train_loss= 0.14321 train_acc= 1.00000 Cross_ent= 0.05716 val_acc= 0.77600 time= 0.05052\n",
      "Epoch: 0084 train_loss= 0.17489 train_acc= 0.98571 Cross_ent= 0.08941 val_acc= 0.77600 time= 0.04525\n",
      "Epoch: 0085 train_loss= 0.13012 train_acc= 0.99286 Cross_ent= 0.04517 val_acc= 0.77400 time= 0.05112\n",
      "Epoch: 0086 train_loss= 0.14909 train_acc= 0.99286 Cross_ent= 0.06470 val_acc= 0.77800 time= 0.06950\n",
      "Epoch: 0087 train_loss= 0.13627 train_acc= 0.98571 Cross_ent= 0.05243 val_acc= 0.77600 time= 0.05212\n",
      "Epoch: 0088 train_loss= 0.15201 train_acc= 0.98571 Cross_ent= 0.06873 val_acc= 0.77600 time= 0.04846\n",
      "Epoch: 0089 train_loss= 0.14431 train_acc= 0.98571 Cross_ent= 0.06158 val_acc= 0.77600 time= 0.05441\n",
      "Epoch: 0090 train_loss= 0.15144 train_acc= 0.98571 Cross_ent= 0.06927 val_acc= 0.77800 time= 0.05203\n",
      "Epoch: 0091 train_loss= 0.14436 train_acc= 1.00000 Cross_ent= 0.06271 val_acc= 0.77800 time= 0.04996\n",
      "Epoch: 0092 train_loss= 0.13750 train_acc= 1.00000 Cross_ent= 0.05637 val_acc= 0.78000 time= 0.04690\n",
      "Epoch: 0093 train_loss= 0.14670 train_acc= 0.98571 Cross_ent= 0.06608 val_acc= 0.78000 time= 0.05492\n",
      "Epoch: 0094 train_loss= 0.14569 train_acc= 0.98571 Cross_ent= 0.06557 val_acc= 0.78200 time= 0.04858\n",
      "Epoch: 0095 train_loss= 0.13667 train_acc= 0.99286 Cross_ent= 0.05702 val_acc= 0.78000 time= 0.05154\n",
      "Epoch: 0096 train_loss= 0.13664 train_acc= 1.00000 Cross_ent= 0.05746 val_acc= 0.78000 time= 0.04501\n",
      "Epoch: 0097 train_loss= 0.16307 train_acc= 0.97143 Cross_ent= 0.08433 val_acc= 0.78000 time= 0.04633\n",
      "Epoch: 0098 train_loss= 0.13905 train_acc= 0.98571 Cross_ent= 0.06072 val_acc= 0.77800 time= 0.04705\n",
      "Epoch: 0099 train_loss= 0.11949 train_acc= 1.00000 Cross_ent= 0.04157 val_acc= 0.77800 time= 0.05919\n",
      "Epoch: 0100 train_loss= 0.13464 train_acc= 0.99286 Cross_ent= 0.05713 val_acc= 0.77800 time= 0.05718\n",
      "Epoch: 0101 train_loss= 0.13608 train_acc= 1.00000 Cross_ent= 0.05897 val_acc= 0.77600 time= 0.05483\n",
      "Epoch: 0102 train_loss= 0.14841 train_acc= 0.99286 Cross_ent= 0.07167 val_acc= 0.77800 time= 0.05877\n",
      "Epoch: 0103 train_loss= 0.14856 train_acc= 0.99286 Cross_ent= 0.07217 val_acc= 0.77800 time= 0.05718\n",
      "Epoch: 0104 train_loss= 0.16397 train_acc= 0.98571 Cross_ent= 0.08793 val_acc= 0.77400 time= 0.04660\n",
      "Epoch: 0105 train_loss= 0.15570 train_acc= 0.97143 Cross_ent= 0.08001 val_acc= 0.77200 time= 0.06037\n",
      "Epoch: 0106 train_loss= 0.13157 train_acc= 0.98571 Cross_ent= 0.05619 val_acc= 0.77600 time= 0.05411\n",
      "Epoch: 0107 train_loss= 0.12285 train_acc= 1.00000 Cross_ent= 0.04778 val_acc= 0.77800 time= 0.05998\n",
      "Epoch: 0108 train_loss= 0.13580 train_acc= 1.00000 Cross_ent= 0.06104 val_acc= 0.77800 time= 0.06309\n",
      "Epoch: 0109 train_loss= 0.11993 train_acc= 0.99286 Cross_ent= 0.04548 val_acc= 0.77800 time= 0.05606\n",
      "Epoch: 0110 train_loss= 0.13643 train_acc= 1.00000 Cross_ent= 0.06227 val_acc= 0.78200 time= 0.05029\n",
      "Epoch: 0111 train_loss= 0.11328 train_acc= 1.00000 Cross_ent= 0.03940 val_acc= 0.78000 time= 0.05007\n",
      "Epoch: 0112 train_loss= 0.14722 train_acc= 0.99286 Cross_ent= 0.07363 val_acc= 0.77800 time= 0.05384\n",
      "Epoch: 0113 train_loss= 0.12888 train_acc= 0.98571 Cross_ent= 0.05556 val_acc= 0.77600 time= 0.05693\n",
      "Epoch: 0114 train_loss= 0.11328 train_acc= 0.99286 Cross_ent= 0.04025 val_acc= 0.77600 time= 0.04902\n",
      "Epoch: 0115 train_loss= 0.12486 train_acc= 0.98571 Cross_ent= 0.05213 val_acc= 0.78000 time= 0.05965\n",
      "Epoch: 0116 train_loss= 0.14060 train_acc= 0.97857 Cross_ent= 0.06817 val_acc= 0.78000 time= 0.05331\n",
      "Epoch: 0117 train_loss= 0.11788 train_acc= 0.99286 Cross_ent= 0.04574 val_acc= 0.77800 time= 0.04888\n",
      "Epoch: 0118 train_loss= 0.11865 train_acc= 0.99286 Cross_ent= 0.04678 val_acc= 0.78000 time= 0.04492\n",
      "Epoch: 0119 train_loss= 0.13820 train_acc= 0.98571 Cross_ent= 0.06661 val_acc= 0.78400 time= 0.05231\n",
      "Epoch: 0120 train_loss= 0.13638 train_acc= 0.98571 Cross_ent= 0.06507 val_acc= 0.78400 time= 0.05267\n",
      "Epoch: 0121 train_loss= 0.13047 train_acc= 0.98571 Cross_ent= 0.05940 val_acc= 0.78200 time= 0.04521\n",
      "Epoch: 0122 train_loss= 0.12759 train_acc= 0.99286 Cross_ent= 0.05675 val_acc= 0.78200 time= 0.05545\n",
      "Epoch: 0123 train_loss= 0.11763 train_acc= 0.99286 Cross_ent= 0.04705 val_acc= 0.78200 time= 0.04592\n",
      "Epoch: 0124 train_loss= 0.11929 train_acc= 1.00000 Cross_ent= 0.04898 val_acc= 0.78400 time= 0.04619\n",
      "Epoch: 0125 train_loss= 0.12722 train_acc= 0.99286 Cross_ent= 0.05718 val_acc= 0.78200 time= 0.05393\n",
      "Epoch: 0126 train_loss= 0.14386 train_acc= 0.99286 Cross_ent= 0.07410 val_acc= 0.78200 time= 0.04870\n",
      "Epoch: 0127 train_loss= 0.12596 train_acc= 0.99286 Cross_ent= 0.05644 val_acc= 0.78800 time= 0.06168\n",
      "Epoch: 0128 train_loss= 0.12867 train_acc= 1.00000 Cross_ent= 0.05941 val_acc= 0.79000 time= 0.05929\n",
      "Epoch: 0129 train_loss= 0.14757 train_acc= 0.98571 Cross_ent= 0.07852 val_acc= 0.78400 time= 0.05523\n",
      "Epoch: 0130 train_loss= 0.12037 train_acc= 0.99286 Cross_ent= 0.05154 val_acc= 0.78000 time= 0.06079\n",
      "Epoch: 0131 train_loss= 0.10873 train_acc= 0.99286 Cross_ent= 0.04011 val_acc= 0.78000 time= 0.06088\n",
      "Epoch: 0132 train_loss= 0.10507 train_acc= 1.00000 Cross_ent= 0.03669 val_acc= 0.78000 time= 0.04598\n",
      "Epoch: 0133 train_loss= 0.13830 train_acc= 0.98571 Cross_ent= 0.07016 val_acc= 0.78000 time= 0.05780\n",
      "Epoch: 0134 train_loss= 0.11700 train_acc= 0.99286 Cross_ent= 0.04908 val_acc= 0.78000 time= 0.04974\n",
      "Epoch: 0135 train_loss= 0.10069 train_acc= 1.00000 Cross_ent= 0.03299 val_acc= 0.78400 time= 0.04771\n",
      "Epoch: 0136 train_loss= 0.09657 train_acc= 1.00000 Cross_ent= 0.02910 val_acc= 0.78000 time= 0.05188\n",
      "Epoch: 0137 train_loss= 0.11596 train_acc= 1.00000 Cross_ent= 0.04874 val_acc= 0.78400 time= 0.04864\n",
      "Epoch: 0138 train_loss= 0.10487 train_acc= 1.00000 Cross_ent= 0.03791 val_acc= 0.78200 time= 0.05422\n",
      "Epoch: 0139 train_loss= 0.11813 train_acc= 0.99286 Cross_ent= 0.05143 val_acc= 0.77800 time= 0.05890\n",
      "Epoch: 0140 train_loss= 0.09706 train_acc= 1.00000 Cross_ent= 0.03061 val_acc= 0.77800 time= 0.06044\n",
      "Epoch: 0141 train_loss= 0.13722 train_acc= 0.98571 Cross_ent= 0.07105 val_acc= 0.77600 time= 0.05824\n",
      "Epoch: 0142 train_loss= 0.11002 train_acc= 0.99286 Cross_ent= 0.04412 val_acc= 0.77400 time= 0.05855\n",
      "Epoch: 0143 train_loss= 0.11137 train_acc= 0.99286 Cross_ent= 0.04573 val_acc= 0.76800 time= 0.04605\n",
      "Epoch: 0144 train_loss= 0.12410 train_acc= 0.98571 Cross_ent= 0.05872 val_acc= 0.76800 time= 0.04778\n",
      "Epoch: 0145 train_loss= 0.10987 train_acc= 0.99286 Cross_ent= 0.04473 val_acc= 0.77000 time= 0.06136\n",
      "Epoch: 0146 train_loss= 0.09657 train_acc= 1.00000 Cross_ent= 0.03166 val_acc= 0.77000 time= 0.06021\n",
      "Epoch: 0147 train_loss= 0.09306 train_acc= 1.00000 Cross_ent= 0.02839 val_acc= 0.77000 time= 0.05552\n",
      "Epoch: 0148 train_loss= 0.11750 train_acc= 0.97857 Cross_ent= 0.05306 val_acc= 0.77000 time= 0.04835\n",
      "Epoch: 0149 train_loss= 0.10918 train_acc= 0.99286 Cross_ent= 0.04499 val_acc= 0.77200 time= 0.05015\n",
      "Epoch: 0150 train_loss= 0.11554 train_acc= 0.98571 Cross_ent= 0.05161 val_acc= 0.77600 time= 0.05094\n",
      "Epoch: 0151 train_loss= 0.13809 train_acc= 0.97857 Cross_ent= 0.07440 val_acc= 0.77400 time= 0.06297\n",
      "Epoch: 0152 train_loss= 0.12132 train_acc= 1.00000 Cross_ent= 0.05786 val_acc= 0.77600 time= 0.04570\n",
      "Epoch: 0153 train_loss= 0.11412 train_acc= 0.99286 Cross_ent= 0.05086 val_acc= 0.77800 time= 0.05744\n",
      "Epoch: 0154 train_loss= 0.11358 train_acc= 0.99286 Cross_ent= 0.05052 val_acc= 0.78000 time= 0.05558\n",
      "Epoch: 0155 train_loss= 0.12435 train_acc= 0.98571 Cross_ent= 0.06147 val_acc= 0.78400 time= 0.05520\n",
      "Epoch: 0156 train_loss= 0.11019 train_acc= 0.99286 Cross_ent= 0.04745 val_acc= 0.78600 time= 0.05338\n",
      "Epoch: 0157 train_loss= 0.10193 train_acc= 1.00000 Cross_ent= 0.03933 val_acc= 0.78200 time= 0.04545\n",
      "Epoch: 0158 train_loss= 0.11127 train_acc= 0.99286 Cross_ent= 0.04881 val_acc= 0.78200 time= 0.04685\n",
      "Epoch: 0159 train_loss= 0.10376 train_acc= 1.00000 Cross_ent= 0.04142 val_acc= 0.78200 time= 0.06538\n",
      "Epoch: 0160 train_loss= 0.12166 train_acc= 0.99286 Cross_ent= 0.05943 val_acc= 0.78400 time= 0.05615\n",
      "Epoch: 0161 train_loss= 0.10329 train_acc= 1.00000 Cross_ent= 0.04115 val_acc= 0.78400 time= 0.05737\n",
      "Epoch: 0162 train_loss= 0.10781 train_acc= 0.99286 Cross_ent= 0.04576 val_acc= 0.78400 time= 0.05784\n",
      "Epoch: 0163 train_loss= 0.11325 train_acc= 0.98571 Cross_ent= 0.05130 val_acc= 0.78200 time= 0.04858\n",
      "Epoch: 0164 train_loss= 0.14051 train_acc= 0.99286 Cross_ent= 0.07866 val_acc= 0.78000 time= 0.05108\n",
      "Epoch: 0165 train_loss= 0.12735 train_acc= 0.98571 Cross_ent= 0.06558 val_acc= 0.77600 time= 0.04784\n",
      "Epoch: 0166 train_loss= 0.08181 train_acc= 1.00000 Cross_ent= 0.02012 val_acc= 0.77400 time= 0.05290\n",
      "Epoch: 0167 train_loss= 0.10864 train_acc= 1.00000 Cross_ent= 0.04705 val_acc= 0.77600 time= 0.06155\n",
      "Epoch: 0168 train_loss= 0.09156 train_acc= 1.00000 Cross_ent= 0.03007 val_acc= 0.77200 time= 0.04531\n",
      "Epoch: 0169 train_loss= 0.10318 train_acc= 0.99286 Cross_ent= 0.04181 val_acc= 0.77200 time= 0.05808\n",
      "Epoch: 0170 train_loss= 0.11671 train_acc= 0.99286 Cross_ent= 0.05547 val_acc= 0.77000 time= 0.05555\n",
      "Epoch: 0171 train_loss= 0.11468 train_acc= 0.98571 Cross_ent= 0.05358 val_acc= 0.77000 time= 0.06142\n",
      "Epoch: 0172 train_loss= 0.09302 train_acc= 1.00000 Cross_ent= 0.03207 val_acc= 0.77200 time= 0.04484\n",
      "Epoch: 0173 train_loss= 0.09170 train_acc= 1.00000 Cross_ent= 0.03090 val_acc= 0.77400 time= 0.05436\n",
      "Epoch: 0174 train_loss= 0.09731 train_acc= 1.00000 Cross_ent= 0.03669 val_acc= 0.77400 time= 0.04846\n",
      "Epoch: 0175 train_loss= 0.11448 train_acc= 0.99286 Cross_ent= 0.05404 val_acc= 0.77600 time= 0.05188\n",
      "Epoch: 0176 train_loss= 0.12521 train_acc= 0.98571 Cross_ent= 0.06495 val_acc= 0.77600 time= 0.05270\n",
      "Epoch: 0177 train_loss= 0.10762 train_acc= 0.99286 Cross_ent= 0.04754 val_acc= 0.78400 time= 0.05381\n",
      "Epoch: 0178 train_loss= 0.13210 train_acc= 0.98571 Cross_ent= 0.07219 val_acc= 0.78400 time= 0.06084\n",
      "Epoch: 0179 train_loss= 0.09872 train_acc= 0.99286 Cross_ent= 0.03900 val_acc= 0.78200 time= 0.05493\n",
      "Epoch: 0180 train_loss= 0.08957 train_acc= 0.99286 Cross_ent= 0.03002 val_acc= 0.78200 time= 0.04647\n",
      "Epoch: 0181 train_loss= 0.10268 train_acc= 1.00000 Cross_ent= 0.04333 val_acc= 0.79000 time= 0.05366\n",
      "Epoch: 0182 train_loss= 0.11024 train_acc= 0.98571 Cross_ent= 0.05107 val_acc= 0.78800 time= 0.05666\n",
      "Epoch: 0183 train_loss= 0.10228 train_acc= 0.99286 Cross_ent= 0.04326 val_acc= 0.79000 time= 0.05523\n",
      "Epoch: 0184 train_loss= 0.09871 train_acc= 1.00000 Cross_ent= 0.03984 val_acc= 0.79000 time= 0.05384\n",
      "Epoch: 0185 train_loss= 0.10908 train_acc= 0.98571 Cross_ent= 0.05033 val_acc= 0.79000 time= 0.05757\n",
      "Epoch: 0186 train_loss= 0.09839 train_acc= 1.00000 Cross_ent= 0.03979 val_acc= 0.78800 time= 0.05916\n",
      "Epoch: 0187 train_loss= 0.12058 train_acc= 1.00000 Cross_ent= 0.06212 val_acc= 0.78800 time= 0.05491\n",
      "Epoch: 0188 train_loss= 0.09974 train_acc= 1.00000 Cross_ent= 0.04139 val_acc= 0.78800 time= 0.05008\n",
      "Epoch: 0189 train_loss= 0.09487 train_acc= 0.99286 Cross_ent= 0.03660 val_acc= 0.78800 time= 0.04914\n",
      "Epoch: 0190 train_loss= 0.10888 train_acc= 0.99286 Cross_ent= 0.05070 val_acc= 0.78600 time= 0.04554\n",
      "Epoch: 0191 train_loss= 0.08714 train_acc= 1.00000 Cross_ent= 0.02904 val_acc= 0.78800 time= 0.05724\n",
      "Epoch: 0192 train_loss= 0.10721 train_acc= 1.00000 Cross_ent= 0.04922 val_acc= 0.78600 time= 0.04986\n",
      "Epoch: 0193 train_loss= 0.10347 train_acc= 0.98571 Cross_ent= 0.04555 val_acc= 0.78800 time= 0.06083\n",
      "Epoch: 0194 train_loss= 0.09328 train_acc= 0.99286 Cross_ent= 0.03544 val_acc= 0.78800 time= 0.05651\n",
      "Epoch: 0195 train_loss= 0.09291 train_acc= 1.00000 Cross_ent= 0.03516 val_acc= 0.78400 time= 0.05788\n",
      "Epoch: 0196 train_loss= 0.11856 train_acc= 0.97857 Cross_ent= 0.06090 val_acc= 0.78600 time= 0.05527\n",
      "Epoch: 0197 train_loss= 0.09543 train_acc= 0.99286 Cross_ent= 0.03787 val_acc= 0.78400 time= 0.05191\n",
      "Epoch: 0198 train_loss= 0.10638 train_acc= 0.98571 Cross_ent= 0.04892 val_acc= 0.77800 time= 0.05015\n",
      "Epoch: 0199 train_loss= 0.09469 train_acc= 1.00000 Cross_ent= 0.03730 val_acc= 0.77400 time= 0.05201\n",
      "Epoch: 0200 train_loss= 0.11293 train_acc= 0.98571 Cross_ent= 0.05561 val_acc= 0.77600 time= 0.05102\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.70706 accuracy= 0.80500 time= 0.01996\n"
     ]
    }
   ],
   "source": [
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function FOR VALIDATION\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(adjfeatures_arr, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy, model.cross_ent], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(adjfeatures_arr, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"Cross_ent=\", \"{:.5f}\".format(outs[3]),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(adjfeatures_arr, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GCN' object has no attribute 'trainable_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d7fa7a2b18c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GCN' object has no attribute 'trainable_variables'"
     ]
    }
   ],
   "source": [
    "print([var.name for var in model.trainable_variables])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'L2Loss_7:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstConvTensor=model.layers[0].vars['weights_0']\n",
    "tf.nn.l2_loss(firstConvTensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn",
   "language": "python",
   "name": "gcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
